{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijGzTHJJUCPY"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEqbX8OhE8y9"
      },
      "source": [
        "# Gemini 1.5: A workshop in multimodal use cases\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/multimodal_use_cases_workshop.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fmultimodal_use_cases_workshop.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/multimodal_use_cases_workshop.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/multimodal_use_cases_workshop.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/multimodal_use_cases_workshop.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/multimodal_use_cases_workshop.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/multimodal_use_cases_workshop.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/multimodal_use_cases_workshop.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/multimodal_use_cases_workshop.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czc2uiFrRIg8"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Katie Nguyen](https://github.com/katiemn) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK1Q5ZYdVL4Y"
      },
      "source": [
        "## Overview\n",
        "\n",
        "### Gemini 1.5 Pro\n",
        "\n",
        "Gemini 1.5 Pro is a new language model from the Gemini family. This model introduces a long context window of up to 1 million tokens that can seamlessly analyze large amounts of information. Additionally, it is multimodal with the ability to process text, images, audio, video, and code. Learn more about [Gemini 1.5 Pro](https://deepmind.google/technologies/gemini/pro/).\n",
        "\n",
        "### Gemini 1.5 Flash\n",
        "\n",
        "This smaller Gemini model is optimized for high-frequency tasks to prioritize the model's response time. This model has superior speed and efficiency with a context window of up to 1 million tokens for all modalities. Learn more about [Gemini 1.5 Flash](https://deepmind.google/technologies/gemini/flash/).\n",
        "\n",
        "In this workshop tutorial, you will learn how to use the Vertex AI SDK for Python to interact with the Gemini 1.5 Pro and Gemini 1.5 Flash models to:\n",
        "  - Cover individual text, PDF, image, video, code, and audio scenarios\n",
        "  - Consider different modality combinations\n",
        "  - Run through an e-commerce use case\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDU0XJ1xRDlL"
      },
      "source": [
        "## Getting Started\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5afkyDMSBW5"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kc4WxYmLSBW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a60e74-bd27-4c93-8da5-0ab8675d126a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /root/.local/lib/python3.10/site-packages (1.74.0)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (3.1.43)\n",
            "Requirement already satisfied: magika in /root/.local/lib/python3.10/site-packages (0.5.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (4.25.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (24.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.13.1)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.10.3)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (0.16)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython) (4.0.11)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from magika) (8.1.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.26 in /usr/local/lib/python3.10/dist-packages (from magika) (1.26.4)\n",
            "Requirement already satisfied: onnxruntime<2.0.0,>=1.17.0 in /root/.local/lib/python3.10/site-packages (from magika) (1.20.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /root/.local/lib/python3.10/site-packages (from magika) (1.0.1)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from magika) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /usr/local/lib/python3.10/dist-packages (from magika) (4.66.6)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
            "Requirement already satisfied: coloredlogs in /root/.local/lib/python3.10/site-packages (from onnxruntime<2.0.0,>=1.17.0->magika) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->magika) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->magika) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.6.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.8.30)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /root/.local/lib/python3.10/site-packages (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->magika) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->magika) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --user google-cloud-aiplatform\\\n",
        "                                        gitpython \\\n",
        "                                        magika"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TgK26CU1N7Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XRvKdaPDTznN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25306fa-6892-4a9c-fab1-0d2e578b5825"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fom0ZkMSBW6"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell to authenticate your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LCaCx6PLSBW6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGB8Txa_e4V0"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JGOJHtgDe5-r"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"gcp-2024-vijay-j\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQwwRiniVFG"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JTk488WDPBtQ"
      },
      "outputs": [],
      "source": [
        "from vertexai.generative_models import GenerativeModel, Image, Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7rZuTClfNs0"
      },
      "source": [
        "### Use the Gemini 1.5 models\n",
        "\n",
        "Gemini 1.5 Pro and Gemini 1.5 Flash are multimodal models that support multimodal prompts. You can include text, image(s), and video in your prompt requests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2998506fe6d1"
      },
      "outputs": [],
      "source": [
        "multimodal_model = GenerativeModel(\"gemini-1.5-pro\")\n",
        "\n",
        "multimodal_model_flash = GenerativeModel(\"gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpL3OkSCfIAR"
      },
      "source": [
        "### Define helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S7QMAHXse339"
      },
      "outputs": [],
      "source": [
        "import http.client\n",
        "import typing\n",
        "import urllib.request\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import IPython.display\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "\n",
        "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
        "    with urllib.request.urlopen(image_url) as response:\n",
        "        response = typing.cast(http.client.HTTPResponse, response)\n",
        "        image_bytes = response.read()\n",
        "    return image_bytes\n",
        "\n",
        "\n",
        "def load_image_from_url(image_url: str) -> Image:\n",
        "    image_bytes = get_image_bytes_from_url(image_url)\n",
        "    return Image.from_bytes(image_bytes)\n",
        "\n",
        "\n",
        "def display_content_as_video(content: str | Image | Part):\n",
        "    if not isinstance(content, Part):\n",
        "        return False\n",
        "    part = typing.cast(Part, content)\n",
        "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
        "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
        "    IPython.display.display(IPython.display.Video(video_url, width=350))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRaGPAnSfeDI"
      },
      "source": [
        "## Individual Modalities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2IKdLXqgEbz"
      },
      "source": [
        "### Textual understanding\n",
        "\n",
        "Gemini 1.5 Pro can parse textual questions and retain that context across following prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EUb1hRkQUPpY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "9cfecc43-49eb-4fab-942d-361c41b7d019"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Mountain View, CA Weather in Mid-May\n\nMountain View enjoys delightful weather in mid-May, with warm days and cool evenings. Expect the following:\n\n* **Temperature:** Highs around 72°F (22°C) and lows around 52°F (11°C).\n* **Sunshine:** Mostly sunny with an average of 9 hours of sunshine per day.\n* **Rainfall:**  May is a dry month with very little rainfall.\n\n## Outfit Suggestions\n\n**Daytime:**\n\n* **Comfortable and Casual:** Embrace the California vibe with a light t-shirt or tank top, paired with jeans, capris, or shorts. \n* **Layering is Key:** A light jacket, cardigan, or sweater is perfect for the cooler mornings and evenings.\n* **Footwear:** Comfortable walking shoes or sandals are ideal for exploring the city.\n* **Accessories:** Don't forget sunglasses and a hat for sun protection. \n\n**Examples:**\n\n* Jeans and a short-sleeved striped shirt with a light denim jacket.\n* Sundress with a cardigan and sandals.\n* Shorts and a flowy top with comfortable walking shoes.\n\n**Evening:**\n\n* **Slightly Dressier:** The evenings can be cool, so bring a light jacket or sweater. A stylish scarf can add a touch of warmth and elegance.\n* **Versatile Options:** Opt for long pants or a skirt with a blouse or a nice top. A maxi dress can also be a comfortable and chic choice.\n* **Footwear:** Depending on the occasion, you can choose from sandals, flats, or even ankle boots.\n\n**Examples:**\n\n* Jeans or black pants with a nice blouse and a light jacket.\n* A flowy maxi dress with a denim jacket and sandals.\n* A skirt with a sweater and ankle boots.\n\n**Remember:** This is just a general guide, and the weather can be unpredictable. It's always a good idea to check the forecast closer to your trip and pack accordingly! \n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "question = \"What is the average weather in Mountain View, CA in the middle of May?\"\n",
        "prompt = \"\"\"\n",
        "Considering the weather, please provide some outfit suggestions.\n",
        "\n",
        "Give examples for the daytime and the evening.\n",
        "\"\"\"\n",
        "\n",
        "contents = [question, prompt]\n",
        "response = multimodal_model.generate_content(contents)\n",
        "display(IPython.display.Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9IkzwizO6h7"
      },
      "source": [
        "### Document Summarization\n",
        "\n",
        "You can use Gemini 1.5 Pro to process PDF documents, and analyze content, retain information, and provide answers to queries regarding the documents.\n",
        "\n",
        "The PDF document example used here is the Gemini 1.5 paper (https://arxiv.org/pdf/2403.05530.pdf).\n",
        "\n",
        "![image.png](https://storage.googleapis.com/cloud-samples-data/generative-ai/image/gemini1.5-paper-2403.05530.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ndhuyoi1PJ4i"
      },
      "outputs": [],
      "source": [
        "pdf_file_uri = \"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\"\n",
        "pdf_file = Part.from_uri(pdf_file_uri, mime_type=\"application/pdf\")\n",
        "\n",
        "prompt = \"How many tokens can the model process?\"\n",
        "\n",
        "contents = [pdf_file, prompt]\n",
        "\n",
        "response = multimodal_model.generate_content(contents)\n",
        "display(IPython.display.Markdown(response.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4bNW5y0WvAV"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "  You are a professional document summarization specialist.\n",
        "  Please summarize the given document.\n",
        "\"\"\"\n",
        "\n",
        "contents = [pdf_file, prompt]\n",
        "\n",
        "response = multimodal_model.generate_content(contents)\n",
        "display(IPython.display.Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OWurhO4mu4J"
      },
      "source": [
        "### Image understanding across multiple images\n",
        "\n",
        "One of Gemini's capabilities is being able to reason across multiple images to provide recommendations.\n",
        "\n",
        "This is an example using Gemini 1.5 Pro to reason which glasses would be more suitable for an oval face shape:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7M7B9q7L1X7"
      },
      "outputs": [],
      "source": [
        "image_glasses1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses1.jpg\"\n",
        "image_glasses2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses2.jpg\"\n",
        "image_glasses1 = load_image_from_url(image_glasses1_url)\n",
        "image_glasses2 = load_image_from_url(image_glasses2_url)\n",
        "\n",
        "prompt = \"\"\"\n",
        "I have an oval face. Given my face shape, which glasses would be more suitable?\n",
        "\n",
        "Explain how you reached this decision.\n",
        "Provide your recommendation based on my face shape, and please give an explanation for each.\n",
        "\"\"\"\n",
        "\n",
        "IPython.display.Image(image_glasses1_url, width=150)\n",
        "IPython.display.Image(image_glasses2_url, width=150)\n",
        "\n",
        "contents = [prompt, image_glasses1, image_glasses2]\n",
        "responses = multimodal_model.generate_content(contents)\n",
        "display(IPython.display.Markdown(responses.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN8nVlITK5kz"
      },
      "source": [
        "### Generating a video description\n",
        "\n",
        "Gemini can also extract tags throughout a video:\n",
        "\n",
        "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT2nArvxZv-P"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "What is shown in this video?\n",
        "Where should I go to see it?\n",
        "What are the top 5 places in the world that look like this?\n",
        "\"\"\"\n",
        "video = Part.from_uri(\n",
        "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "contents = [prompt, video]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents)\n",
        "\n",
        "display_content_as_video(video)\n",
        "display(IPython.display.Markdown(responses.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9RdLpH128Ao"
      },
      "source": [
        "> You can confirm that the location is indeed Antalya, Turkey by visiting the Wikipedia page: https://en.wikipedia.org/wiki/Antalya\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iOXLV14RIhE"
      },
      "source": [
        "You can also use Gemini 1.5 Pro to retrieve extra information beyond the video contents.\n",
        "\n",
        "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CY-zlixU87O"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Which line is this?\n",
        "Where does it go?\n",
        "What are the stations/stops?\n",
        "\"\"\"\n",
        "video = Part.from_uri(\n",
        "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "contents = [prompt, video]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents)\n",
        "\n",
        "display_content_as_video(video)\n",
        "display(IPython.display.Markdown(responses.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZrxMm_83Vps"
      },
      "source": [
        "> You can confirm that this is indeed the Confederation Line on Wikipedia here: https://en.wikipedia.org/wiki/Confederation_Line\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMdpQ_UadvFX"
      },
      "source": [
        "### Reason across a codebase\n",
        "\n",
        "You will use the Online Boutique repo as an example in this notebook. Online Boutique is a cloud-first microservices demo application. The application is a web-based e-commerce app where users can browse items, add them to the cart, and purchase them. This application consists of 11 microservices across multiple languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAdKvFrSd7j0"
      },
      "outputs": [],
      "source": [
        "# The GitHub repository URL\n",
        "repo_url = \"https://github.com/GoogleCloudPlatform/microservices-demo\"  # @param {type:\"string\"}\n",
        "\n",
        "# The location to clone the repo\n",
        "repo_dir = \"./repo\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVGaceVud-Mw"
      },
      "source": [
        "#### Define helper functions for processing GitHub repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZwcjqS_eTqS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "import git\n",
        "import magika\n",
        "\n",
        "m = magika.Magika()\n",
        "\n",
        "\n",
        "def clone_repo(repo_url, repo_dir):\n",
        "    \"\"\"Clone a GitHub repository\"\"\"\n",
        "\n",
        "    if os.path.exists(repo_dir):\n",
        "        shutil.rmtree(repo_dir)\n",
        "    os.makedirs(repo_dir)\n",
        "    git.Repo.clone_from(repo_url, repo_dir)\n",
        "\n",
        "\n",
        "def extract_code(repo_dir):\n",
        "    \"\"\"Create an index, extract content of code/text files\"\"\"\n",
        "\n",
        "    code_index = []\n",
        "    code_text = \"\"\n",
        "    for root, _, files in os.walk(repo_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            relative_path = os.path.relpath(file_path, repo_dir)\n",
        "            code_index.append(relative_path)\n",
        "\n",
        "            file_type = m.identify_path(Path(file_path))\n",
        "            if file_type.output.group in (\"text\", \"code\"):\n",
        "                try:\n",
        "                    with open(file_path) as f:\n",
        "                        code_text += f\"----- File: {relative_path} -----\\n\"\n",
        "                        code_text += f.read()\n",
        "                        code_text += \"\\n-------------------------\\n\"\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    return code_index, code_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWLQSsxzer-T"
      },
      "source": [
        "#### Create an index and extract the contents of a codebase\n",
        "\n",
        "Clone the repo and create an index and extract content of code/text files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3Adn0XBexfo"
      },
      "outputs": [],
      "source": [
        "clone_repo(repo_url, repo_dir)\n",
        "\n",
        "code_index, code_text = extract_code(repo_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXP-FRH7fAFU"
      },
      "source": [
        "#### Define a helper function to generate a prompt to a code related question\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LAvo69HfKql"
      },
      "outputs": [],
      "source": [
        "def get_code_prompt(question):\n",
        "    \"\"\"Generates a prompt to a code related question.\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Questions: {question}\n",
        "\n",
        "    Context:\n",
        "    - The entire codebase is provided below.\n",
        "    - Here is an index of all of the files in the codebase:\n",
        "      \\n\\n{code_index}\\n\\n.\n",
        "    - Then each of the files is concatenated together. You will find all of the code you need:\n",
        "      \\n\\n{code_text}\\n\\n\n",
        "\n",
        "    Answer:\n",
        "  \"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPbgJ2RpfPs7"
      },
      "source": [
        "#### Create a developer getting started guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zkrfto2rfXmy"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Provide a getting started guide to onboard new developers to the codebase.\n",
        "\"\"\"\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "contents = [prompt]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "for response in responses:\n",
        "    IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRwdOWKof3wX"
      },
      "source": [
        "#### Finding bugs in the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO9eR-mdf_co"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Find the top 3 most severe issues in the codebase.\n",
        "\"\"\"\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "contents = [prompt]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "for response in responses:\n",
        "    IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTK8Mm0M_bEd"
      },
      "source": [
        "#### Summarizing the codebase with Gemini 1.5 Flash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKOjC3pj_LTL"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "  Give me a summary of this codebase, and tell me the top 3 things that I can learn from it.\n",
        "\"\"\"\n",
        "\n",
        "prompt = get_code_prompt(question)\n",
        "contents = [prompt]\n",
        "\n",
        "# Generate text using non-streaming method\n",
        "response = multimodal_model_flash.generate_content(contents)\n",
        "IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLhcXQpSo8Gr"
      },
      "source": [
        "### Audio understanding\n",
        "\n",
        "Gemini 1.5 Pro can directly process audio for long-context understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1gnz_4TpD_f"
      },
      "outputs": [],
      "source": [
        "audio_file_path = \"cloud-samples-data/generative-ai/audio/pixel.mp3\"\n",
        "audio_file_uri = f\"gs://{audio_file_path}\"\n",
        "audio_file_url = f\"https://storage.googleapis.com/{audio_file_path}\"\n",
        "\n",
        "IPython.display.Audio(audio_file_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3olx5jGpXCn"
      },
      "source": [
        "#### Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv__QJHjpcKP"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "  Please provide a short summary and title for the audio.\n",
        "  Provide chapter titles, be concise and short, no need to provide chapter summaries.\n",
        "  Provide each of the chapter titles in a numbered list.\n",
        "  Do not make up any information that is not part of the audio and do not be verbose.\n",
        "\"\"\"\n",
        "\n",
        "audio_file = Part.from_uri(audio_file_uri, mime_type=\"audio/mpeg\")\n",
        "contents = [audio_file, prompt]\n",
        "\n",
        "response = multimodal_model.generate_content(contents)\n",
        "IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yajmlHgspheN"
      },
      "source": [
        "#### Transcription using Gemini 1.5 Flash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9eA5TDwpkBg"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "    Can you transcribe this interview, in the format of timecode, speaker, caption.\n",
        "    Use speaker A, speaker B, etc. to identify the speakers.\n",
        "    Please provide each piece of information on a separate bullet point.\n",
        "\"\"\"\n",
        "\n",
        "audio_file = Part.from_uri(audio_file_uri, mime_type=\"audio/mpeg\")\n",
        "contents = [audio_file, prompt]\n",
        "\n",
        "responses = multimodal_model_flash.generate_content(contents)\n",
        "\n",
        "IPython.display.Markdown(responses.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saUaGizqjtsa"
      },
      "source": [
        "## Combining multiple modalities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adXNxPfHqEoE"
      },
      "source": [
        "### Video and audio understanding\n",
        "\n",
        "\n",
        "Try out Gemini 1.5 Pro's native multimodal and long context capabilities on video interleaving with audio inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFT47Rl3qK5Y"
      },
      "outputs": [],
      "source": [
        "video_file_path = \"cloud-samples-data/generative-ai/video/pixel8.mp4\"\n",
        "video_file_uri = f\"gs://{video_file_path}\"\n",
        "video_file_url = f\"https://storage.googleapis.com/{video_file_path}\"\n",
        "\n",
        "IPython.display.Video(video_file_url, width=350)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2bNmx3ZqNqq"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "  Provide a description of the video.\n",
        "  The description should also contain any important dialogue from the video.\n",
        "\"\"\"\n",
        "\n",
        "video_file = Part.from_uri(video_file_uri, mime_type=\"video/mp4\")\n",
        "contents = [video_file, prompt]\n",
        "\n",
        "response = multimodal_model.generate_content(contents)\n",
        "IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-WgKBz8qlRu"
      },
      "source": [
        "### All modalities (images, video, audio, text) at once\n",
        "\n",
        "Gemini 1.5 Pro is natively multimodal and supports interleaving of data from different modalities. It can support a mix of audio, visual, text, and code inputs in the same input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOygXL9Cqq9n"
      },
      "outputs": [],
      "source": [
        "video_file_path = \"cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4\"\n",
        "video_file_uri = f\"gs://{video_file_path}\"\n",
        "video_file_url = f\"https://storage.googleapis.com/{video_file_path}\"\n",
        "\n",
        "IPython.display.Video(video_file_url, width=350)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfwWadcAqtEF"
      },
      "outputs": [],
      "source": [
        "image_file_path = \"cloud-samples-data/generative-ai/image/a-man-and-a-dog.png\"\n",
        "image_file_uri = f\"gs://{image_file_path}\"\n",
        "image_file_url = f\"https://storage.googleapis.com/{image_file_path}\"\n",
        "\n",
        "IPython.display.Image(image_file_url, width=350)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irViZ7zxqvrD"
      },
      "outputs": [],
      "source": [
        "video_file = Part.from_uri(video_file_uri, mime_type=\"video/mp4\")\n",
        "image_file = Part.from_uri(image_file_uri, mime_type=\"image/png\")\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Look through each frame in the video carefully and answer the questions.\n",
        "  Only base your answers strictly on what information is available in the video attached.\n",
        "  Do not make up any information that is not part of the video and do not be too\n",
        "  verbose, be straightforward.\n",
        "\n",
        "  Questions:\n",
        "  - When is the moment in the image happening in the video? Provide a timestamp.\n",
        "  - What is the context of the moment and what does the narrator say about it?\n",
        "\"\"\"\n",
        "\n",
        "contents = [video_file, image_file, prompt]\n",
        "\n",
        "response = multimodal_model.generate_content(contents)\n",
        "IPython.display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAO09UUcvL_L"
      },
      "source": [
        "## Use Case: retail / e-commerce\n",
        "\n",
        "The customer shows you their living room:\n",
        "\n",
        "|Customer photo |\n",
        "|:-----:|\n",
        "|<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/living-room.png\" width=\"50%\">  |\n",
        "\n",
        "\n",
        "\n",
        "Below are four wall art options that the customer is trying to decide between:\n",
        "\n",
        "|Art 1| Art 2 | Art 3 | Art 4 |\n",
        "|:-----:|:----:|:-----:|:----:|\n",
        "| <img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/room-art-1.png\" width=\"60%\">|<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/room-art-2.png\" width=\"100%\">|<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/room-art-3.png\" width=\"60%\">|<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/room-art-4.png\" width=\"60%\">|\n",
        "\n",
        "\n",
        "How can you use Gemini 1.5 Pro, a multimodal model, to help the customer choose the best option?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4joag7vswfYw"
      },
      "source": [
        "### Generating open recommendations\n",
        "\n",
        "Using the same image, you can ask the model to recommend a piece of furniture that would make sense in the space.\n",
        "\n",
        "Note that the model can choose any furniture in this case, and can do so only from its built-in knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z82JDCTiwoxB"
      },
      "outputs": [],
      "source": [
        "# urls for room images\n",
        "room_image_url = \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/living-room.png\"\n",
        "\n",
        "# load room images as Image Objects\n",
        "room_image = load_image_from_url(room_image_url)\n",
        "\n",
        "prompt = \"Describe this room\"\n",
        "contents = [prompt, room_image]\n",
        "\n",
        "IPython.display.Image(room_image_url, width=350)\n",
        "responses = multimodal_model.generate_content(contents)\n",
        "IPython.display.Markdown(responses.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnSwtNKbxizu"
      },
      "outputs": [],
      "source": [
        "prompt1 = \"Recommend a new piece of furniture for this room\"\n",
        "prompt2 = \"Explain the reason in detail\"\n",
        "contents = [prompt1, room_image, prompt2]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents)\n",
        "IPython.display.Markdown(responses.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8XU_0L_xB-_"
      },
      "source": [
        "### Generating recommendations based on provided images\n",
        "\n",
        "Instead of keeping the recommendation open, you can also provide a list of items for the model to choose from. Here, you will download a few art images that the Gemini model can recommend. This is particularly useful for retail companies who want to provide product recommendations to users based on their current setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd05VvNHxGs1"
      },
      "outputs": [],
      "source": [
        "# Download and display sample artwork\n",
        "art_image_urls = [\n",
        "    \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/room-art-1.png\",\n",
        "    \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/room-art-2.png\",\n",
        "    \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/room-art-3.png\",\n",
        "    \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/room-art-4.png\",\n",
        "]\n",
        "\n",
        "# Load wall art images as Image Objects\n",
        "art_images = [load_image_from_url(url) for url in art_image_urls]\n",
        "\n",
        "# To recommend an item from a selection, you will need to label the item number within the prompt.\n",
        "# That way you are providing the model with a way to reference each image as you pose a question.\n",
        "# Labeling images within your prompt also helps reduce hallucinations and produce better results.\n",
        "prompt = \"\"\"\n",
        "  You are an interior designer.\n",
        "  For each piece of wall art, explain whether it would be appropriate for the style of the room.\n",
        "  Rank each piece according to how well it would be compatible in the room.\n",
        "\"\"\"\n",
        "contents = [\n",
        "    \"Consider the following art pieces:\",\n",
        "    \"art 1:\",\n",
        "    art_images[0],\n",
        "    \"art 2:\",\n",
        "    art_images[1],\n",
        "    \"art 3:\",\n",
        "    art_images[2],\n",
        "    \"art 4:\",\n",
        "    art_images[3],\n",
        "    \"room:\",\n",
        "    room_image,\n",
        "    prompt,\n",
        "]\n",
        "\n",
        "IPython.display.Image(room_image_url, width=350)\n",
        "print(\"\\n------Art1:-------\")\n",
        "IPython.display.Image(art_image_urls[0], width=150)\n",
        "print(\"\\n------Art2:-------\")\n",
        "IPython.display.Image(art_image_urls[1], width=150)\n",
        "print(\"\\n------Art3:-------\")\n",
        "IPython.display.Image(art_image_urls[2], width=150)\n",
        "print(\"\\n------Art4:-------\")\n",
        "IPython.display.Image(art_image_urls[3], width=150)\n",
        "\n",
        "responses = multimodal_model.generate_content(contents)\n",
        "IPython.display.Markdown(responses.text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "multimodal_use_cases_workshop.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}